/**
 * Servi√ßo de IA - Centraliza todas as chamadas para OpenAI e Gemini
 */

import { GoogleGenerativeAI } from "@google/generative-ai";
import OpenAI from "openai";
import { readFileSync } from "fs";
import { join, dirname } from "path";
import { fileURLToPath } from "url";
import { config } from "../config/environment.js";
import { sleep } from "../utils/helpers.js";

// Setup __dirname para ESM
const __filename = fileURLToPath(import.meta.url);
const __dirname = dirname(__filename);

// Inicializar clientes de IA
const openai = config.OPENAI_API_KEY 
  ? new OpenAI({ apiKey: config.OPENAI_API_KEY })
  : null;

const genAI = new GoogleGenerativeAI(config.GEMINI_API_KEY);

// ========================= PROMPTS ========================
function loadPrompt(filename) {
  return readFileSync(
    join(__dirname, "..", "prompts", filename),
    "utf-8"
  );
}

export const PROMPTS = {
  PARSER: loadPrompt("system-parser.md"),
  CATEGORY_CLASSIFIER: loadPrompt("system-category-classifier.md"),
  DOCUMENT_ANALYZER: loadPrompt("system-document-analyzer.md"),
  INTENT_CLASSIFIER: loadPrompt("system-intent-classifier.md"),
};

// ========================= LLM CORE ========================

/**
 * Fun√ß√£o principal para executar LLM
 * Tenta Gemini primeiro (gratuito), depois OpenAI como fallback
 * 
 * @param {string} system - Prompt do sistema
 * @param {Array} userParts - Array de partes (texto ou inlineData)
 * @param {number} retryCount - Contador de tentativas
 * @returns {Promise<string>} - Resposta da IA (JSON limpo)
 */
export async function runLLM(system, userParts, retryCount = 0) {
  const maxRetries = 3;
  
  // Tentativa 1: Gemini (PRIORIDADE - cr√©ditos gratuitos)
  try {
    return await runLLMGemini(system, userParts, retryCount);
  } catch (error) {
    console.error(`‚ùå [AI] Erro com Gemini:`, error.message);
    
    // Se Gemini falhou e OpenAI est√° dispon√≠vel, usa como fallback
    if (openai) {
      console.log(`üîÑ [AI] Gemini falhou, tentando OpenAI como fallback...`);
      return runLLMOpenAI(system, userParts);
    }
    
    // Se n√£o tem OpenAI dispon√≠vel, relan√ßa erro do Gemini
    throw error;
  }
}

/**
 * Fun√ß√£o espec√≠fica para OpenAI (fallback quando Gemini falha)
 * 
 * @param {string} system - Prompt do sistema
 * @param {Array} userParts - Array de partes (texto ou inlineData)
 * @returns {Promise<string>} - Resposta da IA (JSON limpo)
 */
async function runLLMOpenAI(system, userParts) {
  try {
    console.log(`ü§ñ [AI] Usando OpenAI (FALLBACK) - Modelo: ${config.OPENAI_MODEL}`);
    
    // Converter userParts para formato OpenAI
    const messages = [
      { role: "system", content: system }
    ];
    
    // Processar userParts (pode ter texto ou inlineData)
    for (const part of userParts) {
      if (part.text) {
        messages.push({ role: "user", content: part.text });
      } else if (part.inlineData) {
        // OpenAI Vision API para imagens/documentos
        const mimeType = part.inlineData.mimeType || "image/jpeg";
        
        if (mimeType.startsWith("image/")) {
          messages.push({
            role: "user",
            content: [
              {
                type: "image_url",
                image_url: {
                  url: `data:${mimeType};base64,${part.inlineData.data}`
                }
              }
            ]
          });
        } else if (mimeType.startsWith("audio/")) {
          // Para √°udio, OpenAI usa Whisper separadamente (n√£o suportado inline)
          throw new Error('OpenAI n√£o suporta √°udio inline. Use Gemini.');
        } else {
          // Para outros tipos, tenta como imagem
          messages.push({
            role: "user",
            content: [
              {
                type: "image_url",
                image_url: {
                  url: `data:${mimeType};base64,${part.inlineData.data}`
                }
              }
            ]
          });
        }
      }
    }
    
    const response = await openai.chat.completions.create({
      model: config.OPENAI_MODEL,
      messages: messages,
      temperature: 0.1, // Mais determin√≠stico para parsing
      max_tokens: 1000
    });
    
    const txt = response.choices[0].message.content.trim();
    console.log(`‚úÖ [AI] Resposta obtida com sucesso usando OpenAI (${config.OPENAI_MODEL})`);
    console.log(`üìä [AI] Tokens usados: ${response.usage.total_tokens} (prompt: ${response.usage.prompt_tokens}, completion: ${response.usage.completion_tokens})`);
    
    // remove cercas ```json
    return txt.replace(/```json|```/g, "").trim();
    
  } catch (error) {
    console.error(`‚ùå [AI] Erro com OpenAI:`, error.message);
    throw error;
  }
}

/**
 * Fun√ß√£o espec√≠fica para Gemini (fallback)
 * 
 * @param {string} system - Prompt do sistema
 * @param {Array} userParts - Array de partes do Gemini
 * @param {number} retryCount - Contador de tentativas
 * @returns {Promise<string>} - Resposta da IA (JSON limpo)
 */
async function runLLMGemini(system, userParts, retryCount = 0) {
  const maxRetries = 2;
  const modelToUse = retryCount === 0 ? config.GEMINI_PRIMARY : config.GEMINI_FALLBACK;
  
  console.log(`ü§ñ [AI] Tentativa Gemini ${retryCount + 1}/${maxRetries + 1} - Modelo: ${modelToUse}`);
  
  try {
    const model = genAI.getGenerativeModel({
      model: modelToUse,
      systemInstruction: system,
    });
    
    const res = await model.generateContent({
      contents: [{ role: "user", parts: userParts }],
    });
    
    const txt = res.response.text().trim();
    console.log(`‚úÖ [AI] Resposta obtida com sucesso usando Gemini (${modelToUse})`);
    
    // remove cercas ```json
    return txt.replace(/```json|```/g, "").trim();
    
  } catch (error) {
    console.error(`‚ùå [AI] Erro com Gemini (${modelToUse}):`, error.message);
    
    // Se √© erro 503 (overloaded) e ainda h√° tentativas
    if (error.message.includes('503') && retryCount < maxRetries) {
      console.log(`üîÑ [AI] Tentando novamente com modelo Gemini fallback...`);
      await sleep(1000);
      return runLLMGemini(system, userParts, retryCount + 1);
    }
    
    // Se √© erro 503 mas j√° esgotou tentativas
    if (error.message.includes('503')) {
      throw new Error('Servi√ßo de IA temporariamente indispon√≠vel. Tente novamente em alguns minutos.');
    }
    
    // Para outros erros, relan√ßa
    throw error;
  }
}

// ========================= FUN√á√ïES ESPEC√çFICAS ========================

/**
 * Classifica a inten√ß√£o do usu√°rio (greeting, create_transaction, view_reports, etc)
 * 
 * @param {string} message - Mensagem do usu√°rio
 * @returns {Promise<{intent: string, confidence: number, extracted_info: string}>}
 */
export async function classifyIntent(message) {
  try {
    console.log(`üéØ [AI-INTENT] Classificando inten√ß√£o da mensagem: "${message.substring(0, 50)}..."`);
    
    const jsonStr = await runLLM(PROMPTS.INTENT_CLASSIFIER, [
      { text: `Mensagem do usu√°rio: "${message}"` }
    ]);
    
    const result = JSON.parse(jsonStr);
    console.log(`‚úÖ [AI-INTENT] Inten√ß√£o identificada: ${result.intent} (confidence: ${result.confidence})`);
    
    return result;
  } catch (error) {
    console.error(`‚ùå [AI-INTENT] Erro ao classificar inten√ß√£o:`, error.message);
    return {
      intent: "unknown",
      confidence: 0.0,
      extracted_info: ""
    };
  }
}

/**
 * Analisa documento e retorna resumo
 * 
 * @param {string} documentText - Texto extra√≠do do documento
 * @returns {Promise<string>} - An√°lise do documento
 */
export async function analyzeDocumentContent(documentText) {
  console.log(`üîç [DOC-ANALYZER] Iniciando an√°lise de documento...`);
  
  const analysisPrompt = `O documento est√° em PT-BR. Fa√ßa uma an√°lise do que est√° contido nele e d√™ um resumo levando em conta as informa√ß√µes ser√£o inseridas em um sistema financeiro.

DOCUMENTO:
${documentText}

Identifique e resuma:
1. Tipo de documento
2. Valores encontrados
3. Datas relevantes
4. Descri√ß√£o do produto/servi√ßo
5. Se √© receita ou despesa
6. Qualquer informa√ß√£o financeira relevante

Seja objetivo e foque em dados que podem virar lan√ßamentos financeiros.`;

  try {
    const analysis = await runLLM(PROMPTS.DOCUMENT_ANALYZER, [
      { text: analysisPrompt }
    ]);
    
    console.log(`‚úÖ [DOC-ANALYZER] An√°lise conclu√≠da: ${analysis.substring(0, 200)}...`);
    return analysis;
    
  } catch (error) {
    console.error(`‚ùå [DOC-ANALYZER] Erro na an√°lise:`, error);
    return "N√£o foi poss√≠vel analisar o documento automaticamente.";
  }
}

/**
 * Encontra melhor categoria usando IA
 * 
 * @param {string} categoriaSugerida - Categoria sugerida pela an√°lise
 * @param {Array} categoriasExistentes - Lista de categorias dispon√≠veis
 * @returns {Promise<string>} - Nome da categoria escolhida
 */
export async function findBestCategory(categoriaSugerida, categoriasExistentes) {
  if (!categoriasExistentes || categoriasExistentes.length === 0) {
    console.log(`‚ùå [AI-CATEGORY] Nenhuma categoria dispon√≠vel`);
    return null;
  }

  if (!categoriaSugerida) {
    console.log(`‚ö†Ô∏è [AI-CATEGORY] Sem categoria sugerida, usando primeira dispon√≠vel`);
    return categoriasExistentes[0];
  }

  const categoriaNames = categoriasExistentes.map(cat => cat.nome);
  
  console.log(`üîç [AI-CATEGORY] Categoria sugerida: "${categoriaSugerida}"`);
  console.log(`üìã [AI-CATEGORY] Categorias dispon√≠veis: ${categoriaNames.join(', ')}`);
  
  const aiPrompt = `categoria_sugerida: ${categoriaSugerida}

categorias_existentes: ${categoriaNames.join(', ')}`;

  try {
    const response = await runLLM(PROMPTS.CATEGORY_CLASSIFIER, [
      { text: aiPrompt }
    ]);
    
    const nomeEscolhido = response.trim();
    console.log(`ü§ñ [AI-CATEGORY] IA escolheu: "${nomeEscolhido}"`);
    
    return nomeEscolhido;
    
  } catch (error) {
    console.error("‚ùå [AI-CATEGORY] Erro na IA para categoria:", error);
    return categoriasExistentes[0]; // fallback
  }
}
